# Predictive Analytics {#sec-pred}

```{r}
#| echo: false
#| warning: false
library(tidyverse)
library(glue)
library(readxl)
library(gt)
library(ggdag)
library(knitr)
box::use(collapse[descr])
source("utils.R")

knit_print.gt <- function(x, ...) {
  stringr::str_c(
    "<div style='all:initial';>\n", 
    gt::as_raw_html(x), 
    "\n</div>"
  ) |> 
    knitr::asis_output()
    
}
registerS3method(
  "knit_print", 'gt_tbl', knit_print.gt, 
  envir = asNamespace("gt") 
  # important to overwrite {gt}s knit_print
)

```

Not yet online

<!-- ### Overfitting -->
<!-- The choice of variables to include in a model is critical. A good model should include variables that are relevant and have a meaningful impact on the dependent variable. Including too many variables can lead to overfitting, where the model captures noise in the data rather than the underlying relationships, making it less accurate for prediction or causal inference. While it is tempting to include as many variables as available in a regression, it is essential to carefully consider which variables to include in the model (again: theory!), as including too many variables can lead to overfitting, which compromises the model's ability to generalize to new data. -->

<!-- Overfitting in the context of a regression model refers to a situation where the model captures not only the underlying relationship between the dependent and independent variables but also the noise or random fluctuations present in the training data. In other words, the model becomes too complex and fits the training data “too closely”, which ultimately results in poor generalization and performance on new, unseen data. It may exhibit high accuracy and low error rates on the training dataset. However, due to its excessive complexity, the model fails to generalize well to new data and may produce inaccurate predictions or high error rates on using new data. Or as one website states: “*It’s like custom-tailoring a suit to a tall, thin individual with especially narrow shoulders and long arms, then hoping to sell it off the rack to the general public.*” (Source: [The Analysis Factor]( https://www.theanalysisfactor.com/overfitting-regression-models/)) -->

<!-- In a model, overfitting can be caused by various factors. One common reason is the inclusion of too many predictor variables or overly complex model structures, such as high-degree polynomial terms. These complexities allow the model to fit the training data very closely, potentially leading to excellent training performance, but at the cost of poor performance on test data. This is because the model has learned the noise in the training data, which is unlikely to be present in the same way in the test data. Another reason for overfitting can be the presence of outliers or other forms of noise in the training data. If a model is highly sensitive to these points, it may skew the model to fit these specific instances closely, resulting in a model that is not representative of the broader dataset. -->

<!-- Common considerations to reduce the risk of overfitting is first of all having a good look at the variables in the model. Only theoretically relevant independent variables should be included in the model to reduce its complexity. In addition, it is always advisable to use larger datasets. A rule of thumb says that per independent variable in the model, the dataset should include at least 10 observations, meaning, if you have 7 independent variables, the dataset should have at least 70 observations (some also say 20 observations per independent variable). Cleaning the data by addressing noise, outliers, and missing values, and ensuring that the dataset is representative of the population of interest also helps to avoid overfitting. This brings us also back to some challenges we explained in @sec-datapat. -->
