# Case 4: Predictive Analysis {#sec-case4}


```{r imports}
#| message: false
library(tidymodels)
library(poissonreg)
library(ggdag)
library(patchwork)
library(collapse, include.only = c("descr"))
source("hhs-ggtheme.R")
```

## Your task: predicting renovation costs

You are working for a hotel chain, Eden resorts. Eden resorts owns 598 hotels in various big cities and tourist destinations all across Europe. You are working on the financial planing for the upcoming year. Right now, you are tasked with predicting the group-level costs for renovating rooms. When looking at the financials of recent years, there seems to be quite a bit of variation across hotels and locations, so you decide to take a closer look at what predicts renovation costs. Not only will it help you with your current planing, but this might be useful information for future expansion plans. 

## Theory as the starting point

You know that you can collect some data from the hotel chain's systems, so you first start sketching out a mental model of what might be important predictors. You start with something like this @fig-hoteldag:

<!-- https://www.erikigelstrom.com/articles/causal-graphs-in-r-with-diagrammer/ -->

```{r}
#| message: false
#| echo: false
#| label: fig-hoteldag
#| fig-cap: "DAG of possible determinants of the N of hotelroom renovations"
dag_sm <- dagify(
  y ~ a + b + c + d + e + f,
  a ~ d,
  b ~ a + e,
  e ~ d,
  c ~ d,
  outcome = "y",
  labels = c(y = "N Renovations", 
             a = "N Rooms", 
             b = "N Guests",
             c = 'Furniture Quality',
             d = 'Location',
             e = 'Type Guests', 
             f = 'Hotel Characteristics ?'),
  coords = list(
    x = c(c = 3, d = 2, a = 1, b = 2, y = 3, e = 2, f = 3),
    y = c(c = 1, d = 1, a = -1, b = -1, y = 0, e = 0, f = -1)
    )
)
ggdag_status(dag_sm, use_labels = "label", text = FALSE) +
  guides(fill = "none", color = "none") +  # Disable the legend
  theme_dag()
```


## The data

It is quite usual to adjust your mental model as you go and learn from what the data tells you. The sketch above is just a rough starting point and you should adjust it, once you start thinking more about the problem. But it is good enough to get started. For now, you use @fig-hoteldag to guide your data collection efforts. You extract the following data from Eden's systems:

```{r datain}
#| message: false
dta <- readr::read_csv("analysisgen/case-hotel-renovations.csv")
dta$price_rank <- ordered(dta$price_rank, levels = 1:5)
dta$hotel_id <- factor(dta$hotel_id)
dta$location_id <- factor(dta$location_id)
glimpse(dta)
```

## Exploratory analysis to get familiar with the data

A good first step is to examine the outcome variable of interest. A histogram to examine to univariate distribution and some descriptive statistics are usually enough to give you a sense of the data:

```{r fig-expo-outcome}
#| fig-cap: "Empirical distribution of the outcome variable"
#| fig-width: 5.5
#| fig-height: 4
dta |> 
  ggplot(aes(x = n_renovations)) + 
  geom_histogram(binwidth = 5) + 
  scale_x_continuous(breaks = scales::breaks_width(10)) +
  scale_y_continuous(expand = expansion(c(0.0,0.05))) +
  theme(panel.grid.minor.x = element_blank()) 
```

```{r}
# here we use the collapse::descr function
descr(dta$n_renovations)
```

## Preparing the data for our comparison of different prediction models

When we compare many different models, we will always run the risk of choosing an overfit model when selecting a model based on *estimates* of out-of-sample performance. Even, though we will use cross-validation (discussed in @sec-pred), we can never really get rid of the fact that we are re-using and re-using the training data and will eventually be fooled by noise. So, we want to keep a bit of the data out of the training routine, so that we can asses the out-of-sample performance of the model we eventually select a bit better. 
We do this via the initial split function. Here we actually use a `group_` split function. The reason is that we have a panel. We have hotel data for multiple years. We want to make sure that we randomly select hotels and *not' hotel-years. By defining `hotel_id` as a grouping variable we make sure that all years of a hotel are in the same split (either in training or in testing).

We could also stratify slightly by the outcome variable. What this does is it sorts `n_renovations` in 3-4 buckets (think: low, medium, high) and ensures that training and test are roughly comparable in their proportions of low, medium, and high number of renovations. This is just a precaution to make sure our testing set is not randomly tilted in terms of the outcome. In our case, we cannot do that however, because we do not have enough data to do such stratification. So we will have to live without

```{r}
set.seed(47)  
splits  <- group_initial_split(dta, prop = 0.8, group = hotel_id)
dta_test  <- testing(splits)
dta_train <- training(splits)
splits
```
Next, we prepare our cross-validation folds. Again, because we have a smallish sample, we use five folds. If we had more, we'd probably go up to ten folds

```{r}
set.seed(48)
folds <- group_vfold_cv(dta_train, v = 5, group = hotel_id)
print(folds)
```

## Deciding on the preprocessing and model steps

Because the number of renovations is a positive integer number, classic and poisson regressions are natural starting point for a predictive analysis. We define both model classes for use later. 

Next, we define our different models. These are a combination of 1) outcome variable, 2) predictors (think x-variables) to use and 3) pre-processing (transformation) steps to be applied to the outcome or predictor variables. Such pre-processing steps could be turning a factor variable into dummy variables, adding squared terms of a particular x variable as an additional predictor, changing the scale of some predictor, and so on. 

Below we just tried a few combinations to illustrate how to set up different models. We start with a recipe that defines what the outcome variables are (the variables to the left of `~`) and what the predictor variables are (the variables to the right of `~`). Then we apply different (pre-processing) steps. 

```{r}
lin_model <- linear_reg(mode = "regression", engine = "lm")
pois_model <- poisson_reg(mode = "regression", engine = "glm")

# basic steps we want in all models:
r0 <- recipe(dta_train) |> 
  step_normalize(all_numeric_predictors()) |> 
  update_role(n_renovations, new_role = "outcome")

# Individual versions of inputs for each model
# Each one updates the preceding version. 
r1 <- r0 |> 
  update_role(c(n_rooms, n_guests), new_role = "predictor")

r2 <- r1 |> 
  update_role(
    c(family_guests, young_tourists, business_traveller),
    new_role = "predictor"
  )

r3 <- r2 |> 
  update_role(price_rank, new_role = "predictor") |> 
  remove_role(c(family_guests, business_traveller), old_role = "predictor") 

r4 <- r3 |> 
  remove_role(c(young_tourists), old_role = "predictor") |> 
  step_dummy(price_rank)
  
r5 <- r4 |> 
  step_poly(n_guests)
```

Once we have defined all our models, we combine them into what tidymodels calls a workflowset. With the `cross = TRUE` option, we will have every combination of model specification and model class as one model in our workflow set. 

```{r}
model_set1 <- 
  workflow_set(
    preproc = list(m1 = r1, m2 = r2, m3 = r3, m4 = r4, m5 = r5),
    models = list(LR = lin_model, PR = pois_model),
    cross = TRUE
  )
```

We now take the defined workflowset and test each model in it on the cross validation folds. Afterwards we use the `rank_results` function to rank the different models according to the root mean squared error metric. We chose RMSE because we are concerned about large errors. We use the verbose option in case there are model hiccups. If there is something going awry (e.g., a rank deficit matrix because we accidentally selected fully co-linear variables) then the verbose option will tell us which work flow had an issue.

```{r}
model_set1_fits <- model_set1 |> 
  workflow_map("fit_resamples", 
               # Options to `workflow_map()`: 
               seed = 1101, verbose = TRUE,
               # Options to `fit_resamples()`: 
               resamples = folds, control = control_resamples(save_pred = TRUE)
               )
```

```{r}
results_ranked <- rank_results(model_set1_fits, rank_metric ="rmse")
results_ranked
```

We can also look at the results in figure form:

```{r fig-predperform1}
#| fig-cap: "Prediction performance of different models in terms of RMSE and R-squared"
#| fig-height: 4
#| fig-width: 7
p1 <- 
  autoplot(model_set1_fits, metric = "rmse") +
  ggrepel::geom_text_repel(aes(label = wflow_id), 
                           nudge_x = 1/8, nudge_y = 1/100, size = 3.5) +
  theme(legend.position = "none")

p2 <- 
  autoplot(model_set1_fits, metric = "rsq") +
  ggrepel::geom_text_repel(aes(label = wflow_id), 
                           nudge_x = 1/8, nudge_y = 1/100, size = 3.5) +
  theme(legend.position = "none")

(p1 + p2)
```



Apparently, the last model with a polynomial specification and price rank dummies does best. Let us take this as our chosen model. We now take it and fit it to the whole training data. And then we will finally assess how well it does on the test sample.


### Fitting the chosen model

We choose the best combination via its workflow id `wflow_id`, fit it to the whole training data this time, and use it to predict the time to repair on the 20% test sample we have not used yet. To better explore the performance of our model it often helps to add our predictions to the test sample:

```{r}
best_wf_id <- results_ranked$wflow_id[1]
chosen_wf <- extract_workflow(model_set1_fits, best_wf_id)

# we want to refit the best model/workflow on the full training set
chosen_wf_fit <- fit(chosen_wf, data = dta_train)

# Now we can use the fitted model to predict on the test set
chosen_wf_pred <- 
  predict(chosen_wf_fit, new_data = dta_test) |> 
  bind_cols(dta_test)

head(chosen_wf_pred)
```

Finally, we can see what the RMSE is on the held-out test data

```{r}
rmse(chosen_wf_pred, truth = n_renovations, estimate = .pred)
```

```{r}
mae(chosen_wf_pred, truth = n_renovations, estimate = .pred)
```

So on average, we are off by 3.6 renovations by year or 5.3 root squared renovations. Given that the mean number of rennovations in the full dataset is `r round(mean(dta$n_renovations), 1)` and the median is `r round(median(dta$n_renovations), 1)` that MAE is ca 23% of the median. So, in terms of magnitude it is still sizable. How expensive this error is, we can only assess once we know the cost of a renovation though. 

We should also always look at a plot of predicted versus actual outcomes. This looks quite decent. We do seem to struggle a bit with the outliers. Also, while a bit hard to see, we seem to not do so well at the very low end ever. We seem to systematically overshoot the hotel-years with a very low number of renovations.

```{r fig-predcomp2}
#| fig-cap: "Predicted values versus actual outcomes"
#| fig-height: 4
#| fig-width: 5
chosen_wf_pred |> 
  ggplot(aes(y = .pred, x = n_renovations, color = n_rooms)) + 
  scale_y_continuous(limits = range(chosen_wf_pred$n_renovations)) + 
  scale_x_continuous(limits = range(chosen_wf_pred$n_renovations)) + 
  geom_abline(intercept = 0, slope = 1, color = "coral1") + 
  geom_point(alpha = 0.9, shape = 21) +
  labs(y = "predicted n_renovations") + 
  theme(legend.position = "right")+ 
  coord_fixed(ratio = 1) 
```

## An alternative set of model with transformed outcomes

Sometimes we can improve predictive performance by transforming the outcome variable. This can sometimes make the functional form tying the outcome variable to predictors more easy to fit. In our case, the number of renovations is obviously heavily dependent on the number of rooms the hotel has. Maybe a better question to ask is: what proportion of hotel rooms is renovated? We can then also frame the number of guests as a proportion of rooms to get to a predictor that reflects how often the average room is used (something like room booking frequency). These transformed variables might be be closer to the underlying mechanism that determines how fast a room "wears out". Let us do these transformations and try everything again:

```{r}
dta2 <- dta |> 
  mutate(
    prop_renov = n_renovations / n_rooms,
    room_freq = n_guests / n_rooms
  )
```
Let's see how the new outcome variable is distributed

```{r fig-expooutcome2}
#| fig-cap: "Empirical distribution of the outcome variable"
#| fig-width: 5.5
#| fig-height: 4
dta2 |> 
  ggplot(aes(x = prop_renov)) + 
  geom_histogram(binwidth = 0.025) + 
  scale_x_continuous(n.breaks = 10) +
  scale_y_continuous(expand = expansion(c(0.01,0.05))) + 
  theme(panel.grid.major.x = element_blank())
```

```{r}
descr(dta2$prop_renov)
```

This is still a pretty skewed distribution. The descriptives show you that we have one year where more than 50% of the rooms were renovated. So, visually it is hard to tell whether this outcome might or might not be hard to get right. Let us carry on nevertheless. It seems a bit easier to reason about it. 

We do our data preparation steps again. This time we do a split that is aware of hotel_id and we also do a cross-validation that is aware of hotel_id. This means, that we effectively randomly choose hotels for the test and training set and for the different folds. This is important because we want to make sure that we do not accidentally train on the same hotel in the training and test set.

```{r}
set.seed(47)
splits2  <- group_initial_split(dta2, prop = 0.8, group = hotel_id)
dta_test2  <- testing(splits2)
dta_train2 <- training(splits2)

set.seed(48)
folds2 <- group_vfold_cv(dta_train2, v = 5, group = hotel_id)
print(folds2)
```

And we define new models

```{r}
r0 <- recipe(dta_train2) |> 
  update_role(prop_renov, new_role = "outcome")

r1 <- r0 |> 
  update_role(room_freq, new_role = "predictor") 

r2 <- r1 |> 
  update_role(c(family_guests, young_tourists, business_traveller), 
              new_role = "predictor")
r3 <- r1 |> 
  update_role(price_rank, new_role = "predictor") |>
  step_dummy(price_rank)

r4 <- r1 |> 
  update_role(price_rank, new_role = "predictor") |>
  step_dummy(price_rank)

r5 <- r3 |> 
  step_sqrt(room_freq)

r6 <- r4 |> 
  step_sqrt(room_freq)

# Here we "accidentally" add another transformation to room_freq after taking
# the square root. This is just to illustrate that you can chain transformations
r7 <- r6 |> 
  step_poly(room_freq)

# Here we leave out the root transformation step
r8 <- r4 |> 
  step_poly(room_freq)

model_set2 <- 
  workflow_set(
    preproc = list(m1 = r1, m2 = r2, m3 = r3, m4 = r4, m5 = r5, 
                   m6 = r6, m7 = r7, m8 = r8),
    models = list(LR = lin_model),
    cross = TRUE
  )

model_set2_fits <- 
  model_set2 |> 
  workflow_map("fit_resamples", 
                # Options to `workflow_map()`: 
               seed = 1101, verbose = TRUE,
               # Options to `fit_resamples()`: 
               resamples = folds2, control = control_resamples(save_pred = TRUE)
               )
```

```{r}
results_ranked2 <- rank_results(model_set2_fits, rank_metric ="rmse")
results_ranked2
```


Because we have transformed the outcome variable, the RMSE is now in terms of renovation proportions and not renovations anymore. So, when we want to compare our new predictions with the old ones we need to create a new variable that is the predicted proportion of renovations times the number of rooms (line 15 below)

```{r}
best_model_id2 <- results_ranked2$wflow_id[1]

chosen_wf2 <- extract_workflow(model_set2, best_model_id2)
chosen_wf_fit2 <- fit(chosen_wf2, data = dta_train2)
chosen_wf_pred2 <- 
  predict(chosen_wf_fit2, new_data = dta_test2) |> 
  bind_cols(dta_test2) |> 
  mutate(pred_renov = n_rooms * .pred)

head(chosen_wf_pred2)
```
Just out of curiosity, do we have any observations, where we predict a negative proportion? (Which would be obviously non-sensical?)

```{r}
chosen_wf_pred2 |> 
  filter(.pred < 0)
```

It does not look like it. That is a good sign for our model fit. For the range of predictors we have in the test sample, we do not get obvious non-sense

```{r}
rmse(chosen_wf_pred2, truth = n_renovations, estimate = pred_renov)
```

```{r}
mae(chosen_wf_pred2, truth = n_renovations, estimate = pred_renov)
```

```{r  fig-predperform2}
#| fig-cap: "Prediction performance of different models in terms of RMSE and R-squared"
#| fig-height: 4
#| fig-width: 7
p1 <- 
  autoplot(model_set2_fits, metric = "rmse") +
  ggrepel::geom_text_repel(aes(label = wflow_id), 
                           nudge_x = 1/8, nudge_y = 1/100, size = 3.5) +
  theme(legend.position = "none")

p2 <- 
  autoplot(model_set2_fits, metric = "rsq") +
  ggrepel::geom_text_repel(aes(label = wflow_id), 
                           nudge_x = 1/8, nudge_y = 1/100, size = 3.5) +
  theme(legend.position = "none")

(p1 + p2)
```

Our transformation did seem to help. We managed to get the RMSE down quite a bit and also slightly reduced the MAE. We can double check the coefficients of our regression model as well to see whether the coefficients are looking reasonable. However, there is an important caveat here. The coefficients might not be intuitively interpretable. That is because we have not designed the regression with isolating specific associations in mind. We only cared about prediction performance when designing the model. You need a DAG again, to see whether and how you can interpret the coefficients. 

```{r}
chosen_wf_fit2 |> tidy()
```

```{r fig-predcomp3}
#| fig-cap: "Predicted values versus actual outcomes"
#| fig-height: 4
#| fig-width: 5
chosen_wf_pred2 |> 
  ggplot(aes(y = pred_renov, x = n_renovations, color = n_rooms)) + 
  scale_y_continuous(limits = range(chosen_wf_pred$n_renovations)) + 
  scale_x_continuous(limits = range(chosen_wf_pred$n_renovations)) + 
  geom_abline(intercept = 0, slope = 1, color = "coral1") + 
  geom_point(alpha = 0.9, shape = 21) +
  labs(y = "predicted n_renovations") + 
  theme(legend.position = "right") + 
  coord_fixed(ratio = 1) 
```

It looks like the transformed regression really helped us doing a better job at the outlying observations (That is also why the RMSE went down more than the MAE). We also seem to do a better job with the hotel-years with low number of observations. This is a common reason why we want to think about transformations in the first place. Whenever we see that we have trouble predicting a specific part of the distribution correctly, we might want to see whether we can transform the distribution to something easier to fit. 


## Takeaway

Similar to our example in @sec-pred, we still need to decide whether we can use the last model for our cost forecasts problem. The individual error still seems large. It is 22% of the median of number of renovations. So, how should we proceed. We could carry on and try to improve the model. Rethink our mental model based on what we learned and maybe, if possible, collect additional predictors that we have not yet considered. Before doing that though, we should first remember that our goal is to forecast group-level renovation costs. So we a) also need costs pre renovation (which likely varies by location and hotel) and b) need to look at the error in agreggate renovations. Because that is ultimately what we want to forecast. If individual errors cancel out, we might do okay with the model we have already. So let us have a look at aggregate errors

```{r}
chosen_wf_pred2 |> 
  summarize(
    agg_n_renov = sum(n_renovations),
    agg_pred_n_renov = sum(pred_renov),
    .by = year
  )
```

That looks quite okay for all the years we have date in our test sample. We need cost per renovation to make the ultimate decision, but this model might be good enough to carry on with your budget planing. 
